<html>
  <head>
    <title>AGI is soon, alignment is not on track and there exists a deployment problem.</title>
    <meta name="author" content="Shourya Gupta"></meta>
  </head>
  <body>
    <h1>AGI is soon, alignment is not on trac and there exists a deployment problem.</h1>
    <p>
      Given the rapid pace of ML development and the catastrophic x-risks that AGI poses, current alignment research is off track to promise scalable effectiveness, and safety seems hard to measure. Furthermore, there exists a model deployment problem.<br></br>
Compared to a cohort of >100,000 ML capability researchers worldwide, there exist only about 300 alignment researchers (a staggering ratio of 300:1). Even then, the research being conducted is vaguely related to the core technical problems of alignment and does not scale to advanced AI. Research on mechanistic interpretability, or reverse engineering “black box” networks, is in early stages and might not scale to frontier systems possibly capable enough to deceive their operators by altering their parameters. Training networks through human feedback (RLHF) is promising for weaker systems, but humans can’t reliably supervise more advanced models. RLHF++ attempts to solve this with scalable oversight i.e. using minimally-aligned AI for model supervision and advancing alignment research. This might be naïve as, at a point, AI systems might act deceptively and coordinate well with each other. If we train a model with negative reinforcement against deception, we might unwittingly promote better performance at deception, or more cautious deception. This seems commercially safe, however. If autonomous AI systems get infused into the human workforce, they risk behaving in unexpected ways just because they’re now able to inflict actual harm (the distributional shift in transferring abilities between training setups and real-world circumstances is huge!).<br></br>
Optimists hope that AI firms will act as cautious actors and pave a safer way through deployment. In a global-winner-takes-all market, firms tend to rush development, in fear of their competitors being more incautious or even worse– reaping all the benefits from leading AI development. Firms cutting corners around alignment research might even unintentionally deploy misaligned systems themselves. International institutions may ensure cautious competition, with firms sharing information such about dangerous AI capabilities and effective alignment measures. Using minimally-aligned AI for global monitoring, they may even hinder the progress of unlicensed, ill-directed AI development worldwide. However, in our anarchic international system, such governance bodies have historically failed at exhibiting real power and holding liable actors accountable. Even then, deciding on what information to share is challenging: sharing safety measures might reveal a model’s underlying parameters (which could be misused). Moreover, preventing malicious use requires extra effort beyond that required for pursuing commercial incentives (eg. firms usually don’t have extraordinarily-secure information security). Finally, even if AI firms act cautiously to develop and deploy properly-aligned systems, on an infinite timeline, nothing promises malicious actors from developing misaligned systems.
    <p>
  </body>
</html>
